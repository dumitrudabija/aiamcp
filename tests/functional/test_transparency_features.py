#!/usr/bin/env python3
"""
Test Transparency Features

Tests the new server introduction tool and visual markers for MCP vs GenAI content.
"""

import json
import subprocess
import sys
import time
from pathlib import Path

def test_transparency_features():
    """Test transparency introduction and visual markers."""

    print("ğŸ” Testing Transparency and MCP vs GenAI Distinction")
    print("=" * 60)

    # Start MCP server
    try:
        server_process = subprocess.Popen(
            [sys.executable, "server.py"],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=str(Path.cwd())
        )

        # Give server time to start
        time.sleep(2)

        # Initialize server
        init_request = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "initialize",
            "params": {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {"name": "transparency-test-client", "version": "1.0.0"}
            }
        }

        server_process.stdin.write(json.dumps(init_request) + "\n")
        server_process.stdin.flush()
        response = server_process.stdout.readline()
        print(f"âœ… Server initialized")

        # Test 1: Server Introduction Tool
        print(f"\nğŸ“‹ Test 1: Server Introduction and Transparency")

        intro_request = {
            "jsonrpc": "2.0",
            "id": 2,
            "method": "tools/call",
            "params": {
                "name": "get_server_introduction",
                "arguments": {}
            }
        }

        server_process.stdin.write(json.dumps(intro_request) + "\n")
        server_process.stdin.flush()
        response_line = server_process.stdout.readline()

        if response_line:
            response = json.loads(response_line.strip())
            if "result" in response:
                content = response["result"]["content"][0]["text"]
                result = json.loads(content)

                print(f"   âœ… Server introduction retrieved successfully")

                # Check key transparency elements
                server_intro = result.get("server_introduction", {})
                print(f"   ğŸ“„ Title: {server_intro.get('title')}")
                print(f"   ğŸ“„ Version: {server_intro.get('version')}")

                transparency = server_intro.get("transparency_notice", {})
                if transparency:
                    print(f"   ğŸ”§ Critical Distinction Present: âœ…")
                    print(f"   ğŸ”§ Data Sources Listed: {len(transparency.get('data_sources', []))}")
                    print(f"   ğŸ”§ Anti-Hallucination Design: âœ…")

                # Check tool categories
                tool_categories = result.get("tool_categories", {})
                print(f"   ğŸ› ï¸  Tool Categories: {len(tool_categories)}")
                for category, info in tool_categories.items():
                    print(f"     - {category}: {len(info.get('tools', []))} tools")

                # Check workflow guidance
                workflow = result.get("workflow_guidance", {})
                if workflow:
                    print(f"   ğŸ“‹ Workflow Guidance: âœ…")
                    print(f"     - Recommended approach: {len(workflow.get('recommended_approach', []))} steps")
                    print(f"     - Automatic features: {len(workflow.get('automatic_features', []))} features")

                # Check compliance warnings
                compliance = result.get("compliance_warnings", {})
                if compliance:
                    print(f"   âš ï¸  Compliance Warnings: âœ…")
                    for warning_type in compliance.keys():
                        print(f"     - {warning_type}: Present")

        # Test 2: Enhanced Tool Response with Visual Markers
        print(f"\nğŸ¯ Test 2: Enhanced Tool Response - Functional Preview")

        functional_preview_request = {
            "jsonrpc": "2.0",
            "id": 3,
            "method": "tools/call",
            "params": {
                "name": "functional_preview",
                "arguments": {
                    "projectName": "Transparency Test AI System",
                    "projectDescription": """
                    This is a machine learning system for automated loan approval decisions that
                    processes financial data including credit scores, income verification, and
                    employment history to make preliminary lending decisions for amounts up to
                    $25,000. The system uses neural networks and decision trees to evaluate
                    risk factors and automatically approves or denies applications based on
                    predefined criteria. The business purpose is to streamline loan processing
                    while maintaining risk management standards. The system impacts customer
                    access to credit and affects our institution's risk exposure. Decisions
                    are made automatically with human review for edge cases above certain
                    risk thresholds. The technical architecture uses cloud-based machine
                    learning services with real-time integration to banking systems and
                    comprehensive audit logging for regulatory compliance.
                    """
                }
            }
        }

        server_process.stdin.write(json.dumps(functional_preview_request) + "\n")
        server_process.stdin.flush()
        response_line = server_process.stdout.readline()

        if response_line:
            response = json.loads(response_line.strip())
            if "result" in response:
                content = response["result"]["content"][0]["text"]
                result = json.loads(content)

                print(f"   âœ… Functional preview with visual markers retrieved")

                # Check for MCP official data section
                mcp_data = result.get("mcp_official_data", {})
                if mcp_data:
                    print(f"   ğŸ”§ MCP OFFICIAL DATA section: âœ…")
                    print(f"     - Data source marked: {mcp_data.get('data_source', 'Missing')}")
                    print(f"     - Risk score: {mcp_data.get('functional_risk_score', 'Missing')}")
                    print(f"     - Impact level: {mcp_data.get('likely_impact_level', 'Missing')}")
                    print(f"     - Scoring methodology: {'âœ…' if 'methodology' in mcp_data.get('scoring_methodology', '') else 'âŒ'}")

                # Check for AI generated analysis section
                ai_analysis = result.get("ai_generated_analysis", {})
                if ai_analysis:
                    print(f"   ğŸ§  AI GENERATED ANALYSIS section: âœ…")
                    print(f"     - Data source marked: {ai_analysis.get('data_source', 'Missing')}")
                    print(f"     - Gap analysis: {'âœ…' if ai_analysis.get('critical_gaps') else 'âŒ'}")
                    print(f"     - Planning guidance: {'âœ…' if ai_analysis.get('planning_guidance') else 'âŒ'}")
                    print(f"     - AI interpretation note: {'âœ…' if 'ai_interpretation_note' in ai_analysis else 'âŒ'}")

                # Check for compliance warnings
                compliance_warnings = result.get("compliance_warnings", {})
                if compliance_warnings:
                    print(f"   âš ï¸  COMPLIANCE WARNINGS section: âœ…")
                    print(f"     - Professional validation: {'âœ…' if 'professional_validation' in compliance_warnings else 'âŒ'}")
                    print(f"     - Regulatory compliance: {'âœ…' if 'regulatory_compliance' in compliance_warnings else 'âŒ'}")

        # Test 3: Verify Visual Markers in Multiple Tools
        print(f"\nğŸ§ª Test 3: Visual Marker Consistency")

        # Test validation tool
        validation_request = {
            "jsonrpc": "2.0",
            "id": 4,
            "method": "tools/call",
            "params": {
                "name": "validate_project_description",
                "arguments": {
                    "projectName": "Test Project",
                    "projectDescription": "A short description for testing validation markers"
                }
            }
        }

        server_process.stdin.write(json.dumps(validation_request) + "\n")
        server_process.stdin.flush()
        response_line = server_process.stdout.readline()

        if response_line:
            response = json.loads(response_line.strip())
            if "result" in response:
                content = response["result"]["content"][0]["text"]
                result = json.loads(content)

                # Check if validation has appropriate markers
                validation_section = result.get("validation", {})
                if validation_section:
                    print(f"   ğŸ” Validation tool transparency: âœ…")
                    print(f"     - Validation results: {'âœ…' if 'is_valid' in validation_section else 'âŒ'}")

        print(f"\nğŸ‰ Transparency Testing Complete!")

        # Summary
        print(f"\nğŸ“‹ Test Summary:")
        print(f"   âœ… Server Introduction: Comprehensive transparency information")
        print(f"   âœ… Visual Markers: MCP vs GenAI content clearly distinguished")
        print(f"   âœ… Compliance Warnings: Professional validation requirements emphasized")
        print(f"   âœ… Data Source Attribution: Official government sources vs AI interpretation")
        print(f"   âœ… Anti-Hallucination Design: Official calculations protected from AI modification")

    except Exception as e:
        print(f"âŒ Test failed with error: {str(e)}")

    finally:
        # Clean up
        if 'server_process' in locals():
            server_process.terminate()
            server_process.wait()

if __name__ == "__main__":
    test_transparency_features()